{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighting prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-guided diffusion models generate images based on a given text prompt. The text prompt\n",
    "can include multiple concepts that the model should generate and it's often desirable to weight\n",
    "certain parts of the prompt more or less. \n",
    "\n",
    "Diffusion models work by conditioning the cross attention layers of the diffusion model with contextualized text embeddings (see the [Stable Diffusion Guide for more information](https://huggingface.co/docs/diffusers/main/en/using-diffusers/../stable-diffusion)).\n",
    "Thus a simple way to emphasize (or de-emphasize) certain parts of the prompt is by increasing or reducing the scale of the text embedding vector that corresponds to the relevant part of the prompt.\n",
    "This is called \"prompt-weighting\" and has been a highly demanded feature by the community (see issue [here](https://github.com/huggingface/diffusers/issues/2431))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do prompt-weighting in Diffusers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe the role of `diffusers` is to be a toolbox that provides essential features that enable other projects, such as [InvokeAI](https://github.com/invoke-ai/InvokeAI) or [diffuzers](https://github.com/abhishekkrthakur/diffuzers), to build powerful UIs. In order to support arbitrary methods to manipulate prompts, `diffusers` exposes a [`prompt_embeds`](https://huggingface.co/docs/diffusers/v0.14.0/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline.__call__.prompt_embeds) function argument to many pipelines such as [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline), allowing to directly pass the \"prompt-weighted\"/scaled text embeddings to the pipeline.\n",
    "\n",
    "The [compel library](https://github.com/damian0815/compel) provides an easy way to emphasize or de-emphasize portions of the prompt for you. We strongly recommend it instead of preparing the embeddings yourself.\n",
    "\n",
    "Let's look at a simple example. Imagine you want to generate an image of `\"a red cat playing with a ball\"` as \n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"a red cat playing with a ball\"\n",
    "\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "\n",
    "image = pipe(prompt, generator=generator, num_inference_steps=20).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you:\n",
    "\n",
    "![img](https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/compel/forest_0.png)\n",
    "\n",
    "As you can see, there is no \"ball\" in the image. Let's emphasize this part!\n",
    "\n",
    "For this we should install the `compel` library:\n",
    "\n",
    "```\n",
    "pip install compel\n",
    "```\n",
    "\n",
    "and then create a `Compel` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compel import Compel\n",
    "\n",
    "compel_proc = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we emphasize the part \"ball\" with the `\"++\"` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a red cat playing with a ball++\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and instead of passing this to the pipeline directly, we have to process it using `compel_proc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds = compel_proc(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass `prompt_embeds` directly to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=\"cpu\").manual_seed(33)\n",
    "\n",
    "images = pipe(prompt_embeds=prompt_embeds, generator=generator, num_inference_steps=20).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the following image which has a \"ball\"!\n",
    "\n",
    "![img](https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/compel/forest_1.png)\n",
    "\n",
    "Similarly, we de-emphasize parts of the sentence by using the `--` suffix for words, feel free to give it \n",
    "a try!\n",
    "\n",
    "If your favorite pipeline does not have a `prompt_embeds` input, please make sure to open an issue, the \n",
    "diffusers team tries to be as responsive as possible.\n",
    "\n",
    "Compel 1.1.6 adds a utility class to simplify using textual inversions.  Instantiate a `DiffusersTextualInversionManager` and pass it to Compel init:\n",
    "\n",
    "```\n",
    "textual_inversion_manager = DiffusersTextualInversionManager(pipe)\n",
    "compel = Compel(\n",
    "    tokenizer=pipe.tokenizer,\n",
    "    text_encoder=pipe.text_encoder,\n",
    "    textual_inversion_manager=textual_inversion_manager)\n",
    "```\n",
    "\n",
    "Also, please check out the documentation of the [compel](https://github.com/damian0815/compel) library for \n",
    "more information."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
